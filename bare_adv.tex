
%% bare_adv.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% See: 
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the advanced use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE Computer
%% Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


% IEEEtran V1.7 and later provides for these CLASSINPUT macros to allow the
% user to reprogram some IEEEtran.cls defaults if needed. These settings
% override the internal defaults of IEEEtran.cls regardless of which class
% options are used. Do not use these unless you have good reason to do so as
% they can result in nonIEEE compliant documents. User beware. ;)
%
%\newcommand{\CLASSINPUTbaselinestretch}{1.0} % baselinestretch
%\newcommand{\CLASSINPUTinnersidemargin}{1in} % inner side margin
%\newcommand{\CLASSINPUToutersidemargin}{1in} % outer side margin
%\newcommand{\CLASSINPUTtoptextmargin}{1in}   % top text margin
%\newcommand{\CLASSINPUTbottomtextmargin}{1in}% bottom text margin




%
\documentclass[10pt,journal,compsoc]{IEEEtran}
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}


% For Computer Society journals, IEEEtran defaults to the use of 
% Palatino/Palladio as is done in IEEE Computer Society journals.
% To go back to Times Roman, you can use this code:
%\renewcommand{\rmdefault}{ptm}\selectfont





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)



% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  %\usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.
\usepackage{url}
\usepackage{hyperref}
%\usepackage[table]{xcolor} 
\usepackage{times}
\usepackage{color}
%\usepackage{multirow} %mk
\usepackage{verbatim}
%\usepackage[draft,inline,nomargin]{fixme}
\usepackage{bbm}

\usepackage{multirow}




% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%\usepackage{acronym}
% acronym.sty was written by Tobias Oetiker. This package provides tools for
% managing documents with large numbers of acronyms. (You don't *have* to
% use this package - unless you have a lot of acronyms, you may feel that
% such package management of them is bit of an overkill.)
% Do note that the acronym environment (which lists acronyms) will have a
% problem when used under IEEEtran.cls because acronym.sty relies on the
% description list environment - which IEEEtran.cls has customized for
% producing IEEE style lists. A workaround is to declared the longest
% label width via the IEEEtran.cls \IEEEiedlistdecl global control:
%
% \renewcommand{\IEEEiedlistdecl}{\IEEEsetlabelwidth{SONET}}
% \begin{acronym}
%
% \end{acronym}
% \renewcommand{\IEEEiedlistdecl}{\relax}% remember to reset \IEEEiedlistdecl
%
% instead of using the acronym environment's optional argument.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/acronym/


%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
%\usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.


% NOTE: PDF thumbnail features are not required in IEEE papers
%       and their use requires extra complexity and work.
\ifCLASSINFOpdf
  \usepackage[pdftex]{thumbpdf}
\else
  \usepackage[dvips]{thumbpdf}
\fi
% thumbpdf.sty and its companion Perl utility were written by Heiko Oberdiek.
% It allows the user a way to produce PDF documents that contain fancy
% thumbnail images of each of the pages (which tools like acrobat reader can
% utilize). This is possible even when using dvi->ps->pdf workflow if the
% correct thumbpdf driver options are used. thumbpdf.sty incorporates the
% file containing the PDF thumbnail information (filename.tpm is used with
% dvips, filename.tpt is used with pdftex, where filename is the base name of
% your tex document) into the final ps or pdf output document. An external
% utility, the thumbpdf *Perl script* is needed to make these .tpm or .tpt
% thumbnail files from a .ps or .pdf version of the document (which obviously
% does not yet contain pdf thumbnails). Thus, one does a:
% 
% thumbpdf filename.pdf 
%
% to make a filename.tpt, and:
%
% thumbpdf --mode dvips filename.ps
%
% to make a filename.tpm which will then be loaded into the document by
% thumbpdf.sty the NEXT time the document is compiled (by pdflatex or
% latex->dvips->ps2pdf). Users must be careful to regenerate the .tpt and/or
% .tpm files if the main document changes and then to recompile the
% document to incorporate the revised thumbnails to ensure that thumbnails
% match the actual pages. It is easy to forget to do this!
% 
% Unix systems come with a Perl interpreter. However, MS Windows users
% will usually have to install a Perl interpreter so that the thumbpdf
% script can be run. The Ghostscript PS/PDF interpreter is also required.
% See the thumbpdf docs for details. The latest version and documentation
% can be obtained at.
% http://www.ctan.org/tex-archive/support/thumbpdf/


% NOTE: PDF hyperlink and bookmark features are not required in IEEE
%       papers and their use requires extra complexity and work.
% *** IF USING HYPERREF BE SURE AND CHANGE THE EXAMPLE PDF ***
% *** TITLE/SUBJECT/AUTHOR/KEYWORDS INFO BELOW!!           ***
\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={Bare Demo of IEEEtran.cls for Computer Society Journals},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Michael D. Shell},%<!CHANGE!
pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper,
             template}}%<^!CHANGE!
%\ifCLASSINFOpdf
%\usepackage[\MYhyperrefoptions,pdftex]{hyperref}
%\else
%\usepackage[\MYhyperrefoptions,breaklinks=true,dvips]{hyperref}
%\usepackage{breakurl}
%\fi
% One significant drawback of using hyperref under DVI output is that the
% LaTeX compiler cannot break URLs across lines or pages as can be done
% under pdfLaTeX's PDF output via the hyperref pdftex driver. This is
% probably the single most important capability distinction between the
% DVI and PDF output. Perhaps surprisingly, all the other PDF features
% (PDF bookmarks, thumbnails, etc.) can be preserved in
% .tex->.dvi->.ps->.pdf workflow if the respective packages/scripts are
% loaded/invoked with the correct driver options (dvips, etc.). 
% As most IEEE papers use URLs sparingly (mainly in the references), this
% may not be as big an issue as with other publications.
%
% That said, Vilar Camara Neto created his breakurl.sty package which
% permits hyperref to easily break URLs even in dvi mode.
% Note that breakurl, unlike most other packages, must be loaded
% AFTER hyperref. The latest version of breakurl and its documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/breakurl/
% breakurl.sty is not for use under pdflatex pdf mode.
%
% The advanced features offer by hyperref.sty are not required for IEEE
% submission, so users should weigh these features against the added
% complexity of use.
% The package options above demonstrate how to enable PDF bookmarks
% (a type of table of contents viewable in Acrobat Reader) as well as
% PDF document information (title, subject, author and keywords) that is
% viewable in Acrobat reader's Document_Properties menu. PDF document
% information is also used extensively to automate the cataloging of PDF
% documents. The above set of options ensures that hyperlinks will not be
% colored in the text and thus will not be visible in the printed page,
% but will be active on "mouse over". USING COLORS OR OTHER HIGHLIGHTING
% OF HYPERLINKS CAN RESULT IN DOCUMENT REJECTION BY THE IEEE, especially if
% these appear on the "printed" page. IF IN DOUBT, ASK THE RELEVANT
% SUBMISSION EDITOR. You may need to add the option hypertexnames=false if
% you used duplicate equation numbers, etc., but this should not be needed
% in normal IEEE work.
% The latest version of hyperref and its documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/hyperref/





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\clubpenalty=10000 
\widowpenalty = 10000
\newcommand{\xhdr}[1]{{\vspace{6pt}\noindent\textbf{\textit{#1}}}}

\newcommand{\soic}{\affaddr{School of Informatics \& Computing}\\ 
\affaddr{Indiana University}\\
\affaddr{Bloomington, IN}\\}




\newcommand{\leaveout}[1]{}


\newcommand{\mustfix}[1]{\fixme{\hl{#1}}}
\newcommand{\pleasenote}[1]{\fxnote{\hl{#1}}}
\newcommand{\hlfixme}[1]{\fixme{\hl{#1}}}
\newcommand{\hlfxnote}[1]{\fxnote{\hl{#1}}}

\newenvironment{packed_itemize}{
\begin{itemize}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
\vspace{-4pt}
}{\end{itemize}}

%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Bare Advanced Demo of IEEEtran.cls\\ for Computer Society Journals}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
        John~Doe,~\IEEEmembership{Fellow,~OSA,}
        and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Shell is with the Department
of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
GA, 30332.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: see http://www.michaelshell.org/contact.html
\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
\thanks{Manuscript received April 19, 2005; revised September 17, 2014.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~13, No.~9, September~2014}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.





% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2014 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2014 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society journal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
\hfill \break
\subsection*{workshop}


The billions of public photos on online social media sites contain a
vast amount of latent visual information about the world.  In this
paper, we study the feasibility of observing the state of the natural
world by recognizing specific types of scenes and objects in
large-scale social image collections. More specifically, we study
whether we can recreate satellite maps of snowfall by automatically
recognizing snowy scenes in geo-tagged, timestamped images from
Flickr.  Snow recognition turns out to be a surprisingly difficult and
under-studied problem, so we test a variety of modern scene
recognition techniques on this problem and introduce a large-scale,
realistic dataset of images with ground truth annotations.  As an
additional proof-of-concept, we test the ability of recognition
algorithms to detect a particular species of flower, the California
Poppy, which could be used to give biologists a new source of data on
its geospatial distribution over time.


\hfill \break
\hfill \break
===================================
\hfill \break
\hfill \break
\subsection*{www}

The popularity of social media websites like Flickr
and Twitter has created enormous collections of user-generated content
online. Latent in these content collections are observations of the
world: each photo is a visual snapshot of what the world looked like
at a particular point in time and space, for example, while each tweet
is a textual expression of the state of a person and his or her
environment. Aggregating these observations across millions
of social sharing users could lead to new techniques for large-scale
monitoring of the state of the world and how it is changing over
time. In this paper we step towards that goal, showing that by
analyzing the tags and image features of geo-tagged, time-stamped
photos we can measure and quantify the occurrence of ecological
phenomena including ground snow cover, snow fall and vegetation
density.  We compare several techniques for dealing with the large
degree of noise in the dataset, and show how machine learning can be
used to reduce errors caused by misleading tags and ambiguous visual
content. We evaluate the accuracy of these techniques by comparing to
ground truth data collected both by surface stations and by
Earth-observing satellites. Besides the immediate application to
ecology, our study gives insight into how to accurately crowd-source                                                                                                                           
other types of information from large, noisy social sharing datasets.

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Computer Society, IEEEtran, journal, \LaTeX, paper, template.
\end{IEEEkeywords}}


% make the title area
\maketitle
% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when compsoc mode
% is not selected <OR> if conference mode is selected - because compsoc
% conference papers position the abstract like regular (non-compsoc)
% papers do!
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc under a non-conference mode.


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\else
\section{Introduction}
\label{sec:introduction}
\fi
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
%% Here we have the typical use of a "T" for an initial drop letter
%% and "HIS" in caps to complete the first word.
%\IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
%for IEEE Computer Society journal papers produced under \LaTeX\ using
%IEEEtran.cls version 1.8a and later.
%% You must have at least 2 lines in the paragraph with the drop letter
%% (should never be an issue)
%I wish you the best of success.
%
%\hfill mds
% 
%\hfill September 17, 2014

\hfill \break
\hfill \break
\subsection*{latest writing}
\hfill \break
\hfill \break

Research has used Twitter and textual social media to track what is going on with people and the world. But images are better -- more information, more faithful, harder to forge, don't have to rely on textual reports. Very little work has used images because they're hard to process automatically. Even the textual work doesn't really consider things at large scale or doesn't measure performance objectively. Here we use images and estimate at continental scale.

We particularly study ecologically related phenomena. Current data is imperfect and ecologists need better data sources, and Flickr could provide that. Current observations are good enough for us to use as ground truth but not perfect. So we can use it to measure our performance but our predictions would still be useful. We choose phenomena that is obvious enough so that scene classification techniques could detect it (as opposed to fine-grained tasks like tracking particular bird species or something).

This is a hard problem. We investigate several techniques to make work better. Metadata (text, timestamps, geotags) eases the problem so we don't have to rely on vision alone. Aggregating information across multiple users means we can make mistakes. Probabilistic confidence interval models the noise explicitly, integrating weak information together. Finally we use deep learning techniques for the vision which are state-of-the-art on classification problems.

\hfill \break
\hfill \break
===================================
\hfill \break
\hfill \break
\subsection*{workshop}

Digital cameras and camera-enabled smartphones are now ubiquitous,
with a large fraction of the population taking photos regularly and
sharing them online. These millions of people taking pictures form a
massive social sensor network that is (in aggregate)
observing and capturing the visual world across time and space.
Modern phones and cameras record metadata like geo-tags and
time-stamps in addition to the images themselves, giving (noisy)
calibration information about how this ad-hoc sensor network is arranged.
%
Social media sites like Flickr and Facebook
thus contain a large amount of latent visual information about
the the world and how it is changing over time. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.42\textwidth,clip,trim=2cm 1cm 1cm 1cm]{figs/natural-images.pdf}
\end{center}
\caption{Many Flickr images contain evidence about the state of the natural world, including
that there is snow on the ground at a particular place and time, that a particular species of bird or animal is present,
and that particular species of plants are flowering.}
\label{fig:nature}
\vspace{-12pt}
\end{figure}

For instance, many (if not most) outdoor images contain some
information about the state of the natural world, such as the weather
conditions and the presence or absence of plants and animals
(Figure~\ref{fig:nature}).  The billions of images on social media
sites could be analyzed to recognize these natural objects and
phenomena, creating a new source of data to biologists and ecologists.
Where are marigolds blooming
today, and how is this geospatial distribution different from a year
ago? Are honeybees less populous this year than last year? Which day
do leaves reach their peak color in each county of the northeastern
U.S.?  These questions can be addressed to some extent by traditional
data collection techniques like satellite instruments, aerial surveys,
or longitudal manual surveys of small patches of land, but none of
these techniques allows scientists to collect fine-grained data at
continental scales: satellites can monitor huge areas of land but cannot 
detect fine-grained features like blooming flowers, while manual surveys
can collect high-quality and fine-grained data only in a small plot of land.
Large-scale analysis of photos on social media
sites could provide an entirely new source of data at a fraction of the cost
of launching a satellite or hiring teams of biologist observers.

The idea of using crowd-sourced data for science and other purposes is
of course not new. Citizen science projects have trained  groups
of volunteers to recognize and report natural phenomena (like bee
counts~\cite{greatsunflower}, bird sightings~\cite{ebirds}, and
snowfall~\cite{king09snowtweets}) near their homes.
Data mining work  has
shown that  social networking sites
like Twitter can 
monitor political opinions~\cite{jin10prediction,digrazia13},
predict financial markets~\cite{bollen11twitter}, track the spread of
disease~\cite{ginsberg09flu}, detect earthquakes~\cite{Sakaki:2010uv}, and monitor weather
conditions~\cite{meteo}. However, the vast majority of this work has
used textual data from micro-blogging sites like Twitter; very few papers have tried to do this with images, despite
the fact that images offer evidence that is richer, less ambiguous,
and much more difficult to fabricate. This is of course because it is
much easier to scan for keywords in Twitter feeds than to
automatically recognize semantic content in huge collections of
images. 
%% In fact, we're aware of only a few papers that have tried to
%% use content analysis of large-scale image collections to observe the
%% physical world. For instance, Leung and Newsam~\cite{Leung:2010wa} used automatic
%% analysis of social image data to infer land use type for a portion of
%% the United Kingdom, while Zhang \textit{et al}~\cite{



In this paper, we test the feasibility of observing the natural world
by recognizing specific types of scenes and objects in large-scale
image collections from social media.  We consider a well-defined but
nevertheless interesting problem: deciding whether there was snow on
the ground at a particular place and on a particular day, given the
set of publicly-available Flickr photos that were geo-tagged and
time-stamped at that place and time. This builds on our early work in
Zhang \textit{et al}~\cite{ecology2012www} which considered a similar
problem, but used only tag information (essentially scanning for
photos that had the tag ``snow'' with some very simple image
processing to remove obvious outliers). Here, we explicitly test
whether large-scale recognition of the image content itself could be
used to do this task.  Of course, snow cover can already be monitored
through satellites and weather stations (although neither of these
data sources is perfect: weather stations are sparse in rural areas
and satellites typically cannot estimate snow cover when it is
cloudy~\cite{modissnow}), so this is not a transformative application
for ecologists in and of itself. Instead, this is an interesting
application for us precisely because fine-grained ground truth is
available, so that we can test the accuracy of crowd-sourced
observations of the natural world, and judge the feasibility of
observing other natural phenomena for which  there are no other possible sources of data.


We initially expected snowy scene recognition to be an easy problem,
in which just looking for large white regions
would work reasonably well.  Surprisingly, amongst the hundreds of
papers on object and scene classification in the literature, we were
surprised to find very few that have explicitly considered detecting
snow. A few papers on scene classification include
snow-related
categories~\cite{XiaoHEOT10,li2007event,li2009totalscene}, while a few
older papers on natural materials
detection~\cite{luo2003spatialcontext,boutell2006semanticfeature}
consider it along with other categories. We test a variety of
recognition techniques on this problem, using a new realistic dataset
of several thousand  images from Flickr with labeled ground
truth.  We find that snow detection in consumer images is 
surprisingly difficult, and we hope this paper and our dataset
will help spark interest in this somewhat overlooked vision problem.
%
We also consider an ecology application where reliable data does not
exist and Flickr image analysis could be potentially quite valuable: estimating the geo-temporal flowering distribution of the
California Poppy.  

%Finally we present some results of applying our
%recognition techniques on geo-tagged and time-stamped images from
%Flickr, in order to estimate the geo-spatial distribution of snow. We
%compare these maps to ground-truth from satellite data.



\hfill \break
\hfill \break
===================================
\hfill \break
\hfill \break
\subsection*{www}


The popularity of social networking websites has grown dramatically
over the last few years, creating enormous collections of
user-generated content online. Photo-sharing sites have become
particularly popular: Flickr and Facebook alone have amassed an
estimated 100 billion images, with over 100 million new images
uploaded every day~\cite{Kremerskothen11}.  People use these
sites to share photos with family and friends, but in the process they
are creating immense public archives of information about the world:
each photo is a record of what the world looked like at a particular
point in time and space.  When combined together, the billions of
photos on these sites combined with metadata including timestamps,
geo-tags, and captions are a rich untapped source of information about
the state of the world and how it is changing over
time.

Recent work has studied how to mine passively-collected data
from social networking and microblogging websites to make estimates
and predictions about world events, including tracking the spread of
disease~\cite{ginsberg09flu}, monitoring for fires and
emergencies~\cite{delongueville09}, predicting product adoption rates
and election outcomes~\cite{jin10prediction}, and estimating aggregate
public mood~\cite{oconnor10mood,bollen11twitter}. In most of these
studies, however, there is either little ground truth available to
judge the quality of the estimates and predictions, or the available
ground truth is an indirect proxy (e.g. since no aggregate public mood
data exists, \cite{oconnor10mood} evaluates against opinion polls,
while~\cite{bollen11twitter} compares to stock market indices).  While
these studies have demonstrated promising results, it is not yet clear
when crowd-sourcing data from social media sites can yield reliable
estimates, or how to deal with the substantial noise and bias in these
datasets. Moreover, these studies have largely focused on textual
content and have not taken advantage of the vast amount of visual
content online.

In this paper, we study the particular problem of estimating
geo-temporal distributions of ecological phenomena using geo-tagged,
time-stamped photos from Flickr.  Our motivations to study this
particular problem are three-fold.  First, biological and ecological
phenomena frequently appear in images, both because photographers take
photos of them purposely (e.g. close-ups of plants and animals) or
incidentally (a bird in the background of a family portrait, or the
snow in the action shot of children sledding).  Second, for the two
phenomena we study here, snowfall and vegetation cover, large-scale
(albeit imperfect) ground truth is available in the form of
observations from satellites and ground-based weather stations.  Thus
we can explicitly evaluate the accuracy of various techniques for
extracting semantic information from large-scale social media
collections.

\begin{figure*}[th]
\begin{center}
\begin{tabular}{ccc}
\textbf{Raw satellite map} & \textbf{Coarsened satellite map} & \textbf{Map estimated by Flickr photo analysis} \\
\fbox{\includegraphics[width=0.23\textheight]{figs/newsat-12212009.png}} &
\fbox{\includegraphics[width=0.23\textheight]{figs/sat-dec212009.png}} &
\fbox{\includegraphics[width=0.23\textheight]{figs/flickr-dec212009.png}} \\
\end{tabular}
\end{center}
\vspace{-6pt}
\caption{Comparing MODIS satellite snow coverage data for North
  America on Dec 21, 2009 with estimates produced by analyzing Flickr
  tags (best viewed on screen in color). \textit{Left:} Original MODIS snow data, where white
  corresponds with water, black is missing data because of cloud
  cover, grey indicates snow cover, and purple indicates no
  significant snow cover.  \textit{Middle:} Satellite data coarsened
  into 1 degree bins, where green indicates snow cover, blue indicates
  no snow, and grey indicates missing data.  \textit{Right:} Estimates
  produced by the Flickr photo analysis proposed in this paper, where
  green indicates high probability of snow cover, and grey and black
  indicate low-confidence areas (with few photos or ambiguous evidence).}
\label{fig:samplemap}
\end{figure*}


Third, while ground truth is available for these particular
phenomena, for other important ecological phenomena (like the geo-temporal distribution of plants and animals) no such data is
available, and social media could help fill this need.
%, and mining data from social media has the potential to fill
%this gap.  
In fact, perhaps no community is in greater need of
real-time, global-scale information on the state of the world than the
scientists who study climate change. Recent work shows that global
climate change is impacting a variety of flora and fauna at local,
regional and continental scales: for example, species of
high-elevation and cold-weather mammals have moved northward, some
species of butterflies have become extinct, waterfowl are losing
coastal wetland habitats as oceans rise, and certain fish populations
are rapidly declining~\cite{ipcc2007climate}. However monitoring these
changes is surprisingly difficult: plot-based studies
involving direct observation of small patches of land yield
high-quality data but are costly and possible only at very small
scales, while aerial surveillance gives data over
large land areas but cloud cover, forests, atmospheric
conditions and mountain shadows can interfere with the observations,
and only certain types of ecological information can be collected from
the air.  To understand how biological phenomena are responding to
both landscape changes and global climate change, ecologists need an
efficient system for ground-based data collection to give detailed
observations across the planet.  A new approach
for creating ground-level, continental-scale datasets is to use
passive data-mining of the huge number of visual observations produced
by millions of users worldwide, in the form of digital images uploaded
to photo-sharing websites.


\xhdr{Challenges.}
There are two key challenges to unlocking the ecological
information latent in these photo datasets. The first is how to recognize 
ecological phenomena appearing in photos and how to map these observations to
specific places and times. Fortunately, modern photo-sharing sites
collect a rich variety of non-visual
information about photos, including metadata recorded by the digital
camera --- exposure settings and timestamps, for example --- as well as
information generated during social sharing  ---
text tags, comments, and ratings, for example. Many sites also
record the
 geographic coordinates of where on Earth a photo was taken, as reported either by a GPS-enabled camera or smartphone, or input manually by the user.
Thus online photos include the ingredients
necessary to produce geo-temporal data about the world,
including information about content (images, tags and comments), and
when (timestamp) and where (geotag) each photo was taken.

The second challenge is how to deal with the biases and noise inherent
in online data. People do not photograph the Earth evenly,
 so there are disproportionate concentrations of
activity near cities and tourist attractions. Photo metadata is often
noisy or inaccurate; for example, users forget to set the clock on
their camera, GPS units fail to find fixes, and users
carelessly tag photos.  Even photos without such errors might be
misleading: the tag ``snow'' on an image might refer to a snow lily or a
snowy owl, while snow appearing in an image might be artificial (as in an indoor zoo exhibit).

\xhdr{This paper.}  In this paper we study how to mine data from
photo-sharing websites to produce crowd-sourced observations of
ecological phenomena.  As a first step towards the longer-term goal of
mining for many types of phenomena, here we study two in particular:
ground snow cover and vegetation cover (``green-up'') data. Both are
critical features for ecologists monitoring the earth's ecosystems.
Importantly for our study, these two phenomena have accurate
fine-grained ground truth available at a continental scale in the form
of observations from aerial instruments like NASA's Terra earth-observing
satellites~\cite{modisveg,modissnow} or networks of ground-based
observing stations run by the U.S. National Weather Service. This
data allows us to evaluate the performance of our crowd-sourced data mining
techniques at a very large scale, including thousands of days of data
across an entire continent.
%This evaluation gives insight into how accurate crowd-sourced 
%observations could be for tracking ecological phenomena for which
%no ground truth exists.
 Using a dataset of nearly 150 million geo-tagged Flickr photos, we
 study whether this data can potentially be a
 reliable resource for scientific research.  An example comparing
 ground truth snow cover data with the estimates produced by our
 Flickr analysis on one particular day (December 21, 2009) is shown in
 Figure~\ref{fig:samplemap}. Note that the Flickr analysis is sparse
 in places with few photographs, while the satellite data is missing
 in areas with cloud cover, but they agree well in areas where both
 observations are present. This (and the much more extensive experimental results presented later in the paper) suggests that Flickr analysis may produce
 useful observations either on its own or as a complement other
 observational sources.


%% More generally, this paper is a step towards answering a more basic
%% question: How reliable could passive mining of social sharing sites be
%% in producing observations of the world?  Analyzing data from social
%% networking and microblogging websites to make estimations and
%% predictions about world events has become a popular research
%% direction, including for example tracking the spread of
%% disease~\cite{ginsberg09flu}, monitoring for fires and other
%% emergencies~\cite{delongueville09}, predicting product adoption and
%% election outcomes~\cite{jin10prediction}, and inferring aggregate
%% public mood~\cite{oconnor10mood,bollen11twitter}. In most of these
%% studies, however, there is either no ground truth to judge the quality
%% of the estimates, or the ground truth that is used is an indirect
%% proxy (e.g. since no aggregate public mood data exists,
%% \cite{oconnor10mood} evaluates against opinion polls,
%% while~\cite{bollen11twitter} compares to stock market indices). In
%% contrast, for predicting some ecological phenomena like vegetation and
%% snow cover, we have daily, dense ground-truth data for the entire
%% globe in the form of satellite observations.

To summarize, the main contributions of this paper include: 

\begin{packed_itemize}
\item[---] introducing the novel idea of mining photo-sharing sites for
  geo-temporal information about ecological phenomena, 
\item[---] introducing several techniques for deriving crowd-sourced
  observations from noisy, biased data using both visual and textual tag analysis, and
\item[---] evaluating the ability of these techniques to accurately measure
  these phenomena, using dense large-scale ground truth.
\end{packed_itemize}






% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

\section{related work}
\label{sec:relatedwork}


A variety of recent work has studied how to apply computational
techniques to analyze online social datasets in order to aid research
in many disciplines~\cite{lazer09}. Much of this work has studied
questions in sociology and human interaction, such as how friendships
form~\cite{feedback08kdd}, how information flows through social
networks~\cite{libennowell08}, how people move through
space~\cite{brockmann06}, and how people influence their
peers\cite{anagnostpopoulos08}.  The goal of these projects is not to
measure data about the physical world itself, but instead to discover
interesting properties of human behavior using social networking sites
as a convenient data source.

\xhdr{Crowd-sourced observational data.}
Other studies have shown the power of social networking sites as a
source of observational data about the world itself.  Bollen
\textit{et al}~\cite{bollen11twitter} use data from Twitter to try to measure
the aggregated emotional state of humanity, computing mood across six
dimensions according to a standard psychological
test. Intriguingly, they find that these changing mood states
correlate well with the Dow Jones Industrial Average, allowing stock
market moves to be predicted up to 3 days in advance.  However their
test dataset is relatively small, consisting of only three weeks of
trading data.  Like us, Jin~\textit{et al}~\cite{jin10prediction} use
Flickr as a source of data for prediction, but they estimate the
adoption rate of consumer photos by monitoring the frequency of tag
use over time. They find that the volume of Flickr tags is 
correlated  with with sales of two products, Macs and iPods. They also
estimate geo-temporal distributions of these sales over time but do
not compare to ground truth, so it is unclear how accurate these
estimates are. In contrast, we evaluate our techniques against a large
ground truth dataset, where the task is to accurately predict the
distribution of a phenomenon (e.g. snow) across an entire continent 
each day for several years.


\xhdr{Crowd-sourcing from social media.} 
Several recent studies have shown the power of social media  for observing
the world itself, as a special case of `social sensing'~\cite{Aggarwal:2013vh}.
This work includes using Twitter data to measure collective emotional
state~\cite{Golder:2011cy} (which, in turn, has found to
be predictive of stock  moves~\cite{bollen11twitter}),
predicting product adoption rates and political election
outcomes~\cite{jin10prediction}, and collecting data about
earthquakes and other natural disasters~\cite{Sakaki:2010uv}.
%Others have tracked the geo-temporal distributions of social media
%tags, query search terms, and English words in large
%datasets~\cite{singh10socialpixels,
%vlachos2004identifying,chien2005semantic,Backstrom:2008tv,Vadrevu2008,ginsberg09flu,Sadilek:2012wp,Cook:2012wj,Michel:2011gm,radinsky2011word,Radinsky:2012ta};

Particularly striking examples include Ginsberg
\textit{et al}~\cite{ginsberg09flu}, who show that 
geo-temporal properties of web search queries can
predict the spread of flu, and Sadilek \textit{et al}~\cite{Sadilek:2012wp} who show that
Twitter feeds can predict when a given person will
fall ill.


\xhdr{Crowd-sourced geo-temporal data.}
Other work has used online data to predict geo-temporal distributions,
but again in domains other than ecology.  Perhaps the most
striking is the work of Ginsberg \textit{et al}~\cite{ginsberg09flu},
who show that by monitoring the geospatial distribution of search
engine queries related to flu symptoms, the spread of the H1N1
flu can be estimated several days before the official statistics produced by traditional
means.
DeLongueville \textit{etal}~\cite{delongueville09} study tweets related to a major fire in
France, but their analysis is at a very small scale (a few dozen
tweets) and their focus is more on human reactions to the fire as
opposed to using these tweets to estimate the fire's position and
severity.  In perhaps the most related existing work to ours,
 Singh \textit{et al}~\cite{singh10socialpixels} create
geospatial heat maps (dubbed ``social pixels'') of various
tags, including snow and greenery, but their focus is on developing a
formal database-style algebra for describing queries on these systems
and for creating visualizations. They do not consider how to produce
accurate predictions from these visualizations, nor do they compare to
any ground truth.

\xhdr{Accuracy of geo and temporal data on Flickr.}
Over a sample of 10 million images on Flickr.com, 37\% (only a small subset) of them probably have incorrect timestamp ~\cite{Whos Time Is It Anyway? yahoo}.
The accuracy of geo-location is limited due to the camera device, and GPS precision.

Meanwhile, a lot of works are trying to correct estimate or correct geo-location of Flickr images.
%~\cite{Geo-Clustering of Images With Missing GeoTags} 
~\cite{singh2010geo}estimates where images are taken for those missing geo-tags. They are optimizing a graph clustering problem. Attributes in their graph include textual tags, timestamps and vision content. It's inspired by an earlier work ~\cite{crandall2009mapping}.
%~\cite{Geo-Location Estimation of Flickr Images: SocialWeb Based Enrichment} 


Timestamp is harder to be accurate due to the same reason of geo-tags and the time-zone problem. 
%In paper %~\cite{Who's Time is it anyway?}, 
Thomee et.al.~\cite{thomee_time2014}  show  a detailed analysis in disagreement of camera time and GPS time. They also estimate a more accurate timestamp when users taking multiple images in a short timespan.
~\cite{hauff2012geo} consider textual meta-data to correct geo tags. 
They also found for users active on both Flickr and Twitter, the Twitter post at around the same time the images are taken can be a reliable reference to estimate the approximate location.



%------------------------------------------------------------


The specific application we consider here is inferring
information about the state of the natural world from social media.
Existing work has analyzed textual content, including text tags and
Twitter feeds, in order to do this. Hyvarinen and
Saltikoff~\cite{meteo} use tag search on Flickr to validate
metereological satellite observations, although the analysis is done
by hand. Zhang \textit{et al}~\cite{ecology2012www} take a large
collection of geo-tagged and time-stamped Flickr photos and search for
snow-related tags to produce estimates of geo-temporal snowfall
distributions, and evaluate them against satellite snow maps.
Singh \textit{et al}~\cite{singh10socialpixels} visualize geospatial
distributions of photos tagged ``snow'' as an example of
their Social Pixels framework, but they study the
database theory needed to perform this analysis
and do not consider the prediction problem.  


Few papers have used actual image content
analysis as we do here. Leung and Newsam~\cite{Leung:2010wa} use scene
analysis in geo-tagged photos to infer land cover and land use types.
Murdock \textit{et al}~\cite{murdock} analyze  geo-referenced
stationary webcam feeds to estimate cloud cover on a day-by-day
basis, and then use these estimates to recreate satellite cloud cover
maps.  Webcams offer a complimentary data source to the social media
images we consider here: on one hand, analyzing webcam data is made
easier by the fact that the camera is stationary and offers dense
temporal resolution; on the other hand, their observations are restricted to 
where public webcams exist, whereas photos on social media sites 
offer a potentially much denser spatial sampling of the world.

We note that these applications are related to citizen science
projects where volunteers across a wide geographic area send 
in observations~\cite{greatsunflower,ebirds,king09snowtweets}. These projects
often use social media, but require observations to be made
explicitly, whereas in our work we ``passively'' analyze social
media feeds generated by untrained and unwitting individuals.

\xhdr{Detecting snow in images} We know of only a handful of
papers that have explicitly considered snow detection in
images. Perhaps the most relevant is the 2003 work of Singhal
\textit{et al}~\cite{singhal2003spatialcontext,luo2003spatialcontext}
which studies this in the context of detecting ``materials'' like
water, grass, sky, etc. They calculate local color and texture
features at each pixel, and then compute a probability
distribution over the materials at each pixel using a neural
network. They partition the image into segments by thresholding these
belief values, and assign a label to each segment with a probabilistic
framework that considers both the beliefs and simple
contextual information like relative location.
 They find that sky and grass
are relatively easy to classify, while snow and water are
most difficult.  Follow-up work~\cite{boutell2006semanticfeature,boutell2005exploiting}
applied more modern techniques like
support vector machines.  Barnum \textit{et al}~\cite{rain2009IJCV}
detect falling snow and rain, a complementary problem
to the one we study here of detecting fallen
snow.

Papers in the scene recognition literature have considered
snowy scenes amongst their scene categories; for instance, Li \textit{et
  al}~\cite{li2009totalscene,li2007event} mention snow as one possible
component of their scene parsing framework, but do not present
experimental results. The SUN database of Xiao \textit{et
  al}~\cite{XiaoHEOT10} includes several snow-related classes like
``snowfield,'' ``ski slope,'' ``ice shelf,'' and ``mountain snowy,''
but other categories like ``residential neighborhood'' sometimes have
snow and sometimes do not, such that detecting these scenes alone
is not sufficient for our purposes. 






\xhdr{Vegetation classification.}
%http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&ved=0CEkQFjAE&url=http%3A%2F%2Fhomes.cs.washington.edu%2F~neeraj%2Fpapers%2Fnk_eccv2012_leafsnap.pdf&ei=W9n8U5z7JoOTyATZloLICA&usg=AFQjCNHzDMozsenIFTdGySrMk0Vw8nhLhA&sig2=CPxNj5C0S-b3YDpR7-jPHg&bvm=bv.73612305,d.aWw

%~\cite{Leafsnap: A Computer Vision System for Automatic Plant Species Identification} 

~\cite{kumar2012leafsnap}identifies plant species by leaf images. They focus on accurate leaf segmentation according to color difference of leaf and background, curvature distribution over scale, and nearest neighbor matching.

%~\cite{Comparisons of Gist Models in Rapid Scene categorization tasks}

~\cite{siagian2008comparison}introduces multiple Gist models in scene classification. There happen to be a test set of vegetation shows Gist feature works great on vegetation classification.

%~\cite{Detecting subpixel woody vegetation in digital imagery using two artificial intelligence approaches}

~\cite{foschi1997detecting}This one may be a little too old
It's using man-made mask and neural networks.
(could be compared with deep learning)

%a lot of works use remote sensing 

%http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&ved=0CDMQFjAC&url=http%3A%2F%2Fwww.researchgate.net%2Fpublication%2F4309074_Greenery_Image_and_Non-greenery_Image_Classification_Using_Adaptive_Neuro-Fuzzy_Inference_System%2Flinks%2F09e4150ddce7a3514d000000&ei=Edr8U6yzJMOAygSg4YGAAw&usg=AFQjCNGXbvsw1sd1aZA1vAnk_f-4n5bwGA&sig2=62HbvxAtNQXCkCH_raYdig&bvm=bv.73612305,d.aWw
This paper ~\cite{balamurugan2007greenery}
%~\cite{greenery and non-greenery image classification using adaptive neuro-guzzy inference system}
is the closest one to our purpose.
They consider
color, texture features in images and get good result. But they only test on a very limited dataset where the positive images are either with one tree in the center or full of trees or meadow. This is inadequate when we are working with   very large number of public shared images.
%200 imgs (50*50)


\xhdr{Citizen science.}
While some volunteer-based biology efforts like the Lost Ladybug
Project~\cite{lostladybug} and the Great Sunflower
Project~\cite{greatsunflower} use social networking sites to
organize and recruit volunteer observers, we are not aware of any
work that has attempted to passively mine ecological data from social media
sites. The visual data in online social networking sites provide a
unique resource for tracking biological phenomena:  because they are
images, this data can be verified in ways that simple text 
cannot.  In addition, the rapidly expanding quantity
of online images with geo-spatial and temporal metadata creates a
fine-scale record of what is happening across the globe.  However, to
unlock the latent information in these vast photo collections, we need
 mining and recognition tools that can efficiently
process large numbers of images, and robust statistical models that
can handle incomplete and incorrect observations.






\section{method}
\label{sec:method}




%///// Need to combine these two parts into one for data set


\subsection{DataSet}
We use a sample of nearly 150 million geo-tagged, timestamped Flickr
photos as our source of user-contributed observational data about the
world. We collected this data using the public Flickr API, by
repeatedly searching for photos within random time periods and
geo-spatial regions, until the entire globe and all days between January 1, 2007 and December 31, 2010 had been covered.
We applied filters to remove blatantly inaccurate
metadata, in particular removing photos with geotag precision less
than about city-scale (as reported by Flickr), and photos whose upload
timestamp is the same as the EXIF camera timestamp (which usually
means that the camera timestamp was missing).  

For ground truth we use large-scale data originating from two
independent sources: ground-based weather stations, and aerial
observations from satellites.  For the ground-based observations, we
use publicly-available daily snowfall and snow depth observations from
the U.S. National Oceanic and Atmospheric Administration (NOAA) Global
Climate Observing System Surface Network (GSN)~\cite{ghcn}.  This data
provides highly accurate daily data, but only at sites that have
surface observing stations. 
%
For denser, more global coverage, we also use
data from the Moderate Resolution Imaging Spectroradiometer (MODIS)
instrument aboard NASA's Terra satellite. The satellite is in a polar
orbit so that it scans the entire surface of the earth every day. The
MODIS instrument measures spectral emissions at various wavelengths,
and then post-processing uses these measurements to estimate ground cover.
In this paper we use two datasets: the daily snow cover
maps~\cite{modissnow} and the two-week vegetation
averages~\cite{modisveg}. Both of these sets of data including an
estimate of the percentage of snow or vegetation ground cover at each
point on earth, along with a quality score indicating the confidence
in the estimate. Low confidence is caused primarily by cloud cover
(which changes the spectral emissions and prevents accurate ground
cover from being estimated), but also by technical problems with the
satellite. 
As an example, Figure~\ref{fig:samplemap} shows raw satellite snow data from one particular day.


\subsubsection{Snow DataSet}
% Create training data to  build snow visual model

The distribution of geo-tagged Flickr photos is highly
non-uniform, with high peaks in population centers and tourist
locations.  Sampling uniformly at random from  Flickr photos
produces a dataset that mirrors this highly non-uniform distribution,
biasing it towards cities and away from rural areas. Since our
eventual goal is to reproduce continental-scale satellite maps, rural
areas are very important. An alternative is biased sampling that
attempts to select more uniformly over the globe, but has the
disadvantage that it no longer reflects the distribution of Flickr
photos. Other important considerations include how to find a variety
of snowy and non-snowy images, including relatively difficult images
that may include wintry scenes with ice but not snow, and how to
prevent highly-active Flickr users from disproportionately affecting
the datasets.

We strike a compromise on these issues by combining together datasets
sampled in different ways.  We begin with a collection of about 100
million Flickr photos geo-tagged within North America and collected
using the public API (by repeatedly querying at different times and
geo-spatial areas, similar to~\cite{hays}). From this set, we
considered only photos taken before January 1, 2009 (so that we could
use later years for creating a separate test set), and selected:
%
%\begin{enumerate}
%\item
(1) all photos tagged \textit{snow,} \textit{snowfall,} \textit{snowstorm,} or \textit{snowy} in
  English and 10 other common languages (about 500,000 images);
%\item
(2) all photos tagged \textit{winter} in English and about 10 other languages (about 500,000 images);
%\item
(3) a random sample of 500,000 images.
%\end{enumerate}
%
This yielded about 1.4 million images after removing duplicates.  We
further sampled from this set in two ways. First, we selected up to 20
random photos from each user, or all photos if a user had less than 20
photos, giving about 258,000 images. Second, we sampled up to 100
random photos from each $0.1^\circ \times 0.1^\circ$
latitude-longitude bin of the earth (roughly 10km $\times$ 10km at the
mid latitudes), yielding about 300,000 images. The combination of
these two datasets has about 425,000 images after removing duplicates,
creating 
a diverse and realistic selection
of images.  We partitioned this dataset into test and training
sets on a per-user basis, so that all of any given user's photos are
in one set or the other  (to reduce the potential
for duplicate images appearing in both
training and test).

We then presented a subset of these images to humans and collected
annotations for each image. We asked people to label
the images into one of four categories: (1) contains obvious
snow near the camera; (2) contains a trace amount of snow near
the camera; (3) contains obvious snow but far away from the
camera (e.g. on a mountain peak); and (4) does not contain snow. 
For our application of reconstructing snowfall maps, we consider (1)
and (2) to be positive classes and (3) and (4) to be negative,
since snowfall in the distance does not give evidence of snow
at the image's geo-tagged location. In total we labeled 10,000 images.

\subsubsection{Vegetation DataSet}
%%% Need to describe the training data for build vegetation visual model


We build a data set with over 10000 images. They are taken before 2009, and are composed by images with "forest" and "summer" like tags and also random images without any tag preference. These images are labeled with categories 
\textit{"Outdoor Greenery","Outdoor non-Greenery","Indoor","Other-modified"},and \textit{"Not available"}.

Finally, we build a positive set with images in category \textit{"Outdoor Greenery"} and a negative set 
with images in categories \textit{"Outdoor non-Greenery"} and \textit{"Indoor"}. To learn a image classification model, we build a training set with 4000 images and a testing set with 1900 images. In training and testing set, there are equal number of positive and negative samples.
To show the diversity of our Flickr image dataset, in figure ~\ref{fig:dataset} we present a random sample of images in our vegetation dataset labeled as positive and negative.









\begin{figure*}[th]
{\small{
\begin{center}
\begin{tabular}{@{}c@{\,\,\,}c@{\,\,\,}c@{\,\,\,}c@{\,\,\,}c@{\,\,\,}}
%\includegraphics[width=0.19\textwidth]{imggrid/datasetposi/1.jpg} &
%\includegraphics[height=1in]{imggrid/datasetposi/2.jpg} &
%\includegraphics[width=0.19\textwidth]{imggrid/datasetposi/3.jpg} &
%\includegraphics[height=1in]{imggrid/datasetposi/4.jpg} &
%\includegraphics[width=0.19\textwidth]{imggrid/datasetposi/5.jpg} \\
%%\multicolumn{5}{c}{(a) Random positive images in vegetation dataset} \\
%\\[-6pt]
%\hline
%\\[-6pt]
\includegraphics[height=1in]{imggrid/datasetposi/6.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/datasetposi/7.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/datasetposi/8.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/datasetposi/9.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/datasetposi/10.jpg} \\
\multicolumn{5}{c}{(a) Random positive images in vegetation dataset} \\ 
%\\[-6pt]
%\hline
%\\[-6pt]
%\includegraphics[width=0.19\textwidth]{imggrid/datasetnega/1.jpg} &
%\includegraphics[width=0.19\textwidth]{imggrid/datasetnega/2.jpg} &
%\includegraphics[width=0.19\textwidth]{imggrid/datasetnega/3.jpg} &
%\includegraphics[width=0.19\textwidth]{imggrid/datasetnega/4.jpg} &
%\includegraphics[width=0.19\textwidth]{imggrid/datasetnega/5.jpg} \\
%\multicolumn{5}{c}{(c) Random false negatives (snow images classified as non-snow)} \\ 
\\[-6pt]
\hline
\\[-6pt]
\includegraphics[height=1in]{imggrid/datasetnega/6.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/datasetnega/7.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/datasetnega/8.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/datasetnega/9.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/datasetnega/10.jpg} \\
\multicolumn{5}{c}{(b) Random negative images in vegetation dataset} \\
\end{tabular}
\end{center}
}}
\caption{Random images from our hand-labeled dataset. Public sharing images are various in quality, contents, illumination and view angle.
Negative images like winter trees without leaves, or indoor images capturing a photo of forest are more confusing.}
\label{fig:dataset}
\end{figure*}


In continental-scale prediction, we only look at images on Flickr.com in year 2007 to 2010, with no tag limitation. We filter out the photos with inaccurate timestamps and geotags. We only use images with geotag precision no less than 13. ( what's that mean?) 

%I paste this from David's email. The only thing is we use geotag precision >= 13 rather than 11. 
%
%[For geotags, there is an
%accuracy number right after the GPS coordinate in the merge\_files; we
%should ignore any photos with a number less than about 11. That means
%that the precision is less than about a city-scale. For timestamps,
%one thing that happens a lot is that people upload photos to Flickr
%that do not have timestamps -- e.g. scanned images. By default, Flickr
%gives this photo a timestamp of the day and time that it was
%*uploaded*. The first date in the merge\_files is the taken time and
%the second is the upload time. So if these two are the same, it means
%the taken time is probably not right and we should ignore the photo.
%Now one complication is that the two timestamps are in different time
%zones -- taken time is in the user's timezone while upload time is in
%Flickr's time zone (pacific time I think). So a simple rule is to
%simply remove any photos whose minutes and seconds are exactly the
%same for both taken time and upload time. That will unnecessarily
%remove about 1/(60*60)=1/3600 photos, but is a good simple rule.]




\subsection{Extracting semantics using tags from individual images}

We consider two learning paradigms. The first
is to produce a single exemplar for each bin in time and space
consisting of the set of all tags used by all users. For each of these
exemplars, the NASA and/or NOAA ground truth data gives a label (snow
or non-snow). We then use standard machine learning algorithms like
Support Vector Machines and decision trees to identify the most
discriminative tags and tag combinations. In the second paradigm, our
goal instead is to classify individual \textit{photos} as containing
snow or not, and then use these classifier outputs to compute the
number of positive and non-positive photos in each bin (i.e., to
compute $m$ and $n$ in the likelihood ratio described in the last
section).





\subsubsection{not using tag in vegetation case}


We tested the performance of using only tags to make vegetation coverage prediction ~\cite{ecology2012www}. The performance shows tag
feature is not going to improve the result of using visual evidence.






\subsection{Extracting semantics using images from individual images}


%\subsubsection{Traditional Visual features}

\subsubsection{Snow Visual features}


Snow is a somewhat unique visual phenomenon, and we claim that
detecting it in images is a unique recognition task. In some cases,
snow can be detected by coarse scene recognition: ski slopes or snowy
landscapes are distinctive scenes. But snow can appear in any kind of
outdoor scene, and is thus like an object. However, unlike most
objects that have some distinctive features, snow is simply a white,
near-textureless material.  (In fact, our informal observation is that
humans detect snow not by recognizing its appearance, but by noticing
that other expected features of a scene are occluded; in this sense,
detecting snow is less about the features that are seen and more about
the features that are \textit{not} seen. We leave this as an
observation to inspire future work.)
%
We tested a variety of off-the-shelf visual features for classifying
whether an image contains fallen snow. We used Support Vector
Machines for classification, choosing kernels based on the feature
type.  Intuitively, color is a very important feature for detecting
snow, and thus we focused on features that use color to some
degree. Our features are:


\xhdr{Color histograms} We begin with perhaps the simplest of color
features. We build joint histograms in CIELAB space, with 4 bins on
the lightness dimension and 14 bins along each of the two color
dimensions, for a total of 784 bins. We experimented with other
quantizations and found that this arrangement worked best.  We encode
the histogram as a 784 dimensional feature and use an SVM with a
chi-squared distance (as in~\cite{XiaoHEOT10}).

\xhdr{Tiny images} 
%Tiny images are another very simple way of
%representing an image that nevertheless capture coarse color and
%spatial layout information
We subsample images to 16 $\times$ 16 pixels, giving 256 pixels per
RGB color plane and yielding a 768 dimensional feature vector.
Drastically reducing the image dimensions yields a feature that is
less sensitive to exact alignment and more computationally
feasible~\cite{torralba2008tiny}.  
%We use an RBF kernel to compare the
%unnormalized distance.


\xhdr{Spatial Moments} Tiny images capture coarse color and spatial
scene layout information, but much information is discarded during
subsampling.  As an alternative approach, we convert the image to LUV
color space, divide it into 49 blocks using a 7 $\times$ 7 grid, and
then compute the mean and variance of each block in each color
channel.  Intuitively, this is a low-resolution image and a very
simple texture feature, respectively.  We also compute maximum,
minimum, and median value within each cell, so that the final
feature vector has 735 dimensions.

%<<<<<<< .mine

%\begin{figure*}[th!]
%\begin{center}
%\vspace{-16pt}
%\begin{tabular}{cc}
% \includegraphics[width=0.4\textwidth]{figs/ROC-curves.jpg} &
%\includegraphics[width=0.4\textwidth]{figs/PR-curves.jpg} \\
%\end{tabular}
%\end{center}
%\vspace{-12pt}
%\caption{
%Snow classification results for different features and combinations, in terms of {\textit{(left):}} ROC curves for the task of classifying snow vs. non-snow images; and 
%{\textit{(right):}} Precision-Recall curves for the task of retrieving snow images.
%}
%\label{fig:PR_ROC_snow}
%\end{figure*}


\xhdr{{Color Local Binary Pattern (LBP) with pyramid
    pooling}} LBP represents each $9 \times 9$ pixel neighborhood 
as an 8-bit binary number by thresholding the 8 outer pixels
by the value at the center.  We build 256-bin histograms over
these LBP values, both on the grayscale image and on each RGB color
channel~\cite{korayem2012solving}. We compute these histograms in each cell of a three-level spatial
pyramid, with 1 bin at the lowest level, 4 bins in a $2 \times 2$ grid
at the second level, and 16 bins in a $4 \times 4$ grid at the third level.
This yields a $(1+4+16) \times 4 \times 256$ = 21504 dimensional feature
vector for each image.

\xhdr{GIST} We also apply GIST features, which capture coarse texture
and scene layout by applying a Gabor filter bank followed by
down-sampling~\cite{oliva2001modeling}. Our variant produces a
1536-dimensional  vector and operates on color planes. Scaling
images to have square aspect ratios before computing GIST improved
classification results significantly~\cite{douze2009evaluation}.


We experimented with a number of other features, and found
that they did not work well; local features like SIFT and HOG in
particular perform poorly, again because snow does not have
distinctive local visual appearance. 

\subsubsection{Vegetation visual features}

Vegetation has the signature green color. 
The leaves of plants have distinctive visual texture. 
So we employ SIFT feature to analyze the local gradient distribution. 
And we also extract GIST feature to describe texture feature and global context. 


\textbf{\textit{Color SIFT histogram.}}
We extract dense SIFT feature on each of the RGB color plane, and concatenate them to build color SIFT feature. The dense SIFT feature is extracted from every 2 pixels by 2 pixels bin, with a step size of 5 pixels. In this way, we achieve representative key points and reasonable computation complex. 
% Therefore we have 128*3-d for each key point
% about 300-500 key points each images

From training data set, We build 2000 dimensional centers of color SIFT feature using K-means clustering. With these centers, a 2000 dimensional histogram is built from all the key points of each image.

Using SIFT histogram, a model is trained and tested with SVM using RBF kernel. 
%Maybe the numbers should be in evaluation section        
%The performance is 78.10\%.

\textbf{\textit{Color GIST.}}
Similar to color SIFT feature, we also extract GIST feature from each of the RGB color plane.\\*\\*
%The performance is 82.58\%.

\textbf{\textit{Combine visual features.}}
The combined visual feature is built from concatenating the normalized GIST feature and SIFT histogram. A new model is learnt based on the combined feature.
%The performance is 85.9\%.

\subsubsection{Deep learning}
Recently the Conventional Neural Network (CNN) ~\cite{krizhevsky2012imagenet} has gained a lot of attention in the vision community, as it outperformed all other techniques in the ImageNet challenge (the most famous object category detection competition)~\cite{ilsvrcarxiv14}.



CNN is a special type of feed forward neural network inspired by the biological process~\cite{krizhevsky2012imagenet} in cats' visual cortex. 
%It is currently the de facto algorithm of image classification on standard datasets.
CNN enjoys additional features that distinguish it from the standard neural networks: shared wights and sparse connectivity.
A layer in CNN may consist of three different stages: convolution, non-linear activation, and pooling.
In the convolution stage, a set of convolution filters is applied in parallel. 
The output of a convolution filter is then passed to non-linear activation functions (e.g., rectified linear activation function, sigmoid activation function).
The final stage is pooling, where the net output is manipulated based on its neighbors (e.g., max pooling , $L_2$ norm, and weighted average).
Pooling makes the network invariant to the translation of the input

The key idea behind this approach is that instead of first designing low-level features by
hand and then running a machine learning algorithm, a single unified
algorithm should learn both the low-level features and the
high-classifier simultaneously. 

We apply CNN to detect snow and vegetation on image level. We followed Oquab~\cite{Oquab14} et al. and started with a model pre-trained on the huge ImageNet dataset then we train our models using hand-labeled data sets.
 



\subsection{Combining evidence together across users}

Our goal is to estimate the presence or absence of a given ecological
phenomenon (like a species of plant or flower, or a meteorological
feature like snow) on a given day and at a given place,
using only the geo-tagged, time-stamped photos from Flickr. One way of viewing
this problem is that every time a user takes a photo of a phenomenon
of interest, they are casting a ``vote''  that the
phenomenon actually occurred in a given geospatial region. 
 We could
simply look for tags indicating the presence of a feature --
i.e. count the number of photos with the tag ``snow'' --  
but sources of noise and bias make this task 
challenging, including:
\begin{packed_itemize}
\item[---] \textit{Sparse sampling:} The geospatial distribution of photos
  is highly non-uniform. A lack of photos
  of a phenomenon in a region does not
  necessarily mean that it was not there. 
\item[---] \textit{Observer bias:} Social media users are younger and
  wealthier than average, and most live in North
  America and Europe.
\item[---] \textit{Incorrect, incomplete and misleading tags:}
  Photographers may use incorrect or ambiguous tags  ---
  e.g. the tag ``snow'' may refer to a snowy owl or interference on a
  TV screen.
\item[---] \textit{Measurement errors:} Geo-tags and timestamps are
  often incorrect (e.g. because people   forget to set their camera clocks).
\end{packed_itemize}

\xhdr{A statistical test.}  We introduce a simple probabilistic model
and use it to derive a statistical test that can deal with some such
sources of noise and bias. The test could be used for estimating the
presence of any phenomenon of interest; without loss of generality we
use the particular case of snow here, for ease of explanation.  Any
given photo either contains evidence of snow (event $s$) or does not
contain evidence of snow (event $\bar{s}$).  We assume that a given
photo taken at a time and place with snow has a fixed probability $P(s
| snow)$ of containing evidence of snow; this probability is less than
1.0 because many photos are taken indoors, and outdoor photos might be
composed in such a way that no snow is visible. We also assume that
photos taken at a time and place without snow have some non-zero
probability $P(s | \overline{snow})$ of containing evidence of snow;
this incorporates various scenarios including incorrect timestamps or
geo-tags and misleading visual evidence (e.g.  man-made
snow).

Let $m$ be the number of snow
photos (event $s$), and $n$ be the number of non-snow photos (event
$\bar{s}$) taken at a place and time of interest. Assuming that each photo is captured
independently, we can use Bayes' Law to
derive the probability that a given place has snow
given its number of snow and non-snow photos,
%
%\newcommand{\smsn}{\overbrace{s\cdots s}^{m},\overbrace{\overline{s}\cdots \overline{s}}^{n}}
%%hp cr: \bar{s}^n
\newcommand{\smsn}{s^m, \bar{s}^n}
\newcommand{\smsntwo}{s^m, \bar{s}^n}
%%\newcommand{\smsn}{s^m, s^n}
%%\newcommand{\smsntwo}{s^m, s^n}
%\newcommand{\smsntwo}{\underbrace{s\cdots s}_{m},\underbrace{\overline{s}\cdots \overline{s}}_{n}}
\begin{eqnarray*}
P(snow|\smsn)  &=&\frac{ P(\smsn|snow)P(snow)}{P(\smsntwo)}  \\
&=&\frac{{m+n\choose m}p^{m}(1-p)^{n}P(snow)}{P(\smsntwo)},  
\end{eqnarray*}
%
where we write $s^m, \bar{s}^n$ to denote $m$ occurrences of event $s$ and $n$ occurrences of event $\bar{s}$, and where $p=P(s|snow)$ and $P(snow)$ is the prior probability of snow. A similar derivation gives the posterior probability that the bin does not contain snow,
%
\begin{eqnarray*}
P(\overline{snow}|\smsn)  &=&\frac{{m+n\choose m}q^{m}(1-q)^{n}P(\overline{snow})}{P(\smsntwo)},  
\end{eqnarray*}
%
where $q=P(s|\overline{snow})$. 
%
Taking the ratio between these two posterior probabilities yields a likelihood ratio,
%
\begin{eqnarray}
\frac{P(snow|\smsn)}{P(\overline{snow}|\smsntwo)}
%\\=\frac{\frac{{m+n\choose m}p^{m}(1-p)^{n}P(snow)}{P(\overbrace{s\cdots s}^{m},\overbrace{\overline{s}\cdots \overline{s}}^{n})}}{\frac{{m+n\choose m}q^{m}(1-q)^{n}P(\overline{snow})}{P(\overbrace{s\cdots s}^{m},\overbrace{\overline{s}\cdots \overline{s}}^{n})}}
&=&\frac{P(snow)}{P(\overline{snow})}\left(\frac{p}{q}\right)^{m}\left(\frac{1-p}{1-q}\right)^n.
\label{eq:conf}
\end{eqnarray}
%
This ratio can be thought of as a measure of the confidence that a
given time and place actually had snow, given photos from Flickr.

A simple way of classifying a photo into a positive event $s$ or a
negative event $\bar{s}$ is to use text tags. We identify a
set ${\cal S}$ of tags related to a phenomenon of
interest. Any photo tagged with at least one tag in ${\cal S}$ is
declared to be a positive event $s$, and otherwise it is considered a
negative event $\bar{s}$. For the snow detection task, we use the set
${\cal S}$=\{snow, snowy, snowing, snowstorm\}, which we selected
by hand.

%%, which we chose by
%%looking at the 200 most frequent Flickr tags and hand selecting those
%%directly relevant to snowfall.

The above derivation assumes that photos are taken independently of
one another, which is generally not true in reality. One particular
source of dependency is that photos from the same user are highly
correlated with one another.  To mitigate this problem, instead of
counting $m$ and $n$ as numbers of \textit{photos}, we instead let $m$ be 
the number of \textit{photographers} having at least one photo with evidence of snow,
while $n$ is the numbers of photographers who did not upload any
photos with evidence of snow.

The probability parameters in the likelihood ratio of
equation~(\ref{eq:conf}) can be directly estimated from training data
and ground truth. For example, for the snow cover results
presented in Section~\ref{sec:results}, the learned parameters are: $p
= p(s|snow) = 17.12\%$, $q = p(s|\overline{snow}) = 0.14\%$.  In other
words, almost 1 of 5 people at a snowy place take a photo containing
snow, whereas about 1 in 700 people take a photo containing evidence
of snow at a non-snowy place.

Figure~\ref{fig:samplemap} shows a visualization of the likelihood
ratio values for the U.S. on one particular day using this simple
technique with ${\cal S}$=\{snow, snowy, snowing, snowstorm\}.  High
likelihood ratio values are plotted
in green, indicating a high confidence of snow in a geospatial bin,
while low values are shown in blue and indicate high confidence of 
no snow.  Black areas indicate a likelihood ratio 
near 1, showing little conference either way, and grey areas lack
data entirely (having no Flickr photos in that bin on that day).



%\xhdr{Temporal smoothing.}  For many phenomena (including snow), the
%existence of an event on one day is strongly correlated with its
%existence on the next day. Thus one way of addressing the sparsity of
%Flickr photos in some locations is to propagate evidence forward and
%backward in time.  To do this, we apply a Gaussian filter on the Flickr
%confidence values for each bin in an attempt to achieve better
%recalls. We vary the degree of smoothing by using Gaussians with
%different variance values.  We tried smoothing with many
%different parameters, including smoothing both forward and backwards
%in time, or in only one direction.

% I really suggest we take this off. Not get any help from time and location data, and not do any 
%post-process to is a selling point of our paper. And we are not doing this after WWW paper
%any more.


\xhdr{Voting.}  Voting is an interesting
technique  because of its simplicity. Voting
simply counts the number of users who have annotated at least one
photo in a given bin and day with a snow-related tag.
Figure~\ref{fig:precvotes}
plots precision versus the number of votes for snow retrieval.  The
shape of these curve illustrates why crowd-sourced observations of the
world can be reliable, if enough people are involved: as the number of
votes for snow increases, it becomes progressively less likely that
these independent observations are coincidental, and more likely that
they are caused by the presence or absence of an actual phenomenon.
It is interesting to notice that when there are 7 or more snow voters,
snow prediction precision becomes 100\%, while the same is true for
non-snow prediction when the number of non-snow voters reaches 33
if there are no snow voters in the bin.

\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.3\textwidth,trim=1cm 0.5cm 1cm 1cm,clip]{plots/precvotes.png}
%\includegraphics[width=0.3\textwidth,trim=1cm 1cm 1cm 1cm,clip]{plots/precvotesnosnow.png} \\
\end{tabular}
\end{center}
\vspace{-20pt}
\caption{Precision vs number of votes for snow predictions using the voting method.}
\label{fig:precvotes}
\end{figure}
%\hfill \break
%\hfill \break
%\subsection*{latest writing}
%\hfill \break
%\hfill \break


%Three parts:
%
%A. Extracting semantics using tags from individual images, which is what we did in WWW paper. Discuss simple keyword search on e.g. a few snow words, or machine learning to find keyword combinations. 
%
%B. Extracting semantics using images from individual images. Here we talk about the traditional features that are covered in ICCV paper, and then the deep learning techniques.
%
%C. Combining evidence together across users. **Here we have the simple voting method and the probabilistic confidence score. We can augment with the additional factors that Stefan suggested, including priors based on time of year or geographic location, or other evidence like historical accuracy of specific users.] Also including temporal and spatial smoothing by simply adding to the confidence score priors.
%
%[Note what I am omitting here. My preference would be to ignore any techniques that try to classify bins jointly by aggregating all the tags or visual features of photos together in a bin and then using those features. I would also really like if the final classification is based just on thresholding a confidence score, even if the results are slightly worse than the best we can do. I just think it makes the story more complicated to do otherwise and harder to justify.] 
%
%\hfill \break
%\hfill \break
%===================================
%\hfill \break
%\hfill \break






\begin{comment}

%\subsection{Datasets}
\begin{comment}
We use a sample of nearly 150 million geo-tagged, timestamped Flickr
photos as our source of user-contributed observational data about the
world. We collected this data using the public Flickr API, by
repeatedly searching for photos within random time periods and
geo-spatial regions, until the entire globe and all days between January 1, 2007 and December 31, 2010 had been covered.
We applied filters to remove blatantly inaccurate
metadata, in particular removing photos with geotag precision less
than about city-scale (as reported by Flickr), and photos whose upload
timestamp is the same as the EXIF camera timestamp (which usually
means that the camera timestamp was missing).  

For ground truth we use large-scale data originating from two
independent sources: ground-based weather stations, and aerial
observations from satellites.  For the ground-based observations, we
use publicly-available daily snowfall and snow depth observations from
the U.S. National Oceanic and Atmospheric Administration (NOAA) Global
Climate Observing System Surface Network (GSN)~\cite{ghcn}.  This data
provides highly accurate daily data, but only at sites that have
surface observing stations. 
%
For denser, more global coverage, we also use
data from the Moderate Resolution Imaging Spectroradiometer (MODIS)
instrument aboard NASA's Terra satellite. The satellite is in a polar
orbit so that it scans the entire surface of the earth every day. The
MODIS instrument measures spectral emissions at various wavelengths,
and then post-processing uses these measurements to estimate ground cover.
In this paper we use two datasets: the daily snow cover
maps~\cite{modissnow} and the two-week vegetation
averages~\cite{modisveg}. Both of these sets of data including an
estimate of the percentage of snow or vegetation ground cover at each
point on earth, along with a quality score indicating the confidence
in the estimate. Low confidence is caused primarily by cloud cover
(which changes the spectral emissions and prevents accurate ground
cover from being estimated), but also by technical problems with the
satellite. 
As an example, Figure~\ref{fig:samplemap} shows raw satellite snow data from one particular day.

\end{comment}


%% \begin{figure}
%% \begin{center}
%% \begin{tabular}{cc}
%% \includegraphics[width=0.5\textwidth]{plots/nasadownsample20070101.png}
%% \end{tabular}
%% \end{center}
%% \caption{Down-sampled NASA MODIS snow coverage data for North America
%%   on 2007.01.01. Grey: water-covered, or no data, or the confidence
%%   index value is 0; blue: no snow and has non-zero confidence index
%%   value; green: has snow and non-zero confidence index value. (Please
%%   view in color.)}
%% \label{fig:nasadownsample20070101}
%% \end{figure}








\section{experiments and results}
\label{sec:experiments}



\subsection{Snow Case}

As noted above, we are aware of very little work that has considered
the problem of detecting snow in images: the most relevant
work~\cite{singhal2003spatialcontext} considers snow in the context of
natural materials classification, but is over 10 years old, uses a
small and biased dataset, and does not report classification results.
Recent work on scene understanding~\cite{XiaoHEOT10} sometimes
includes snow-related scenes, but none of this work applies directly
to our problem because snow can appear across a range of different
scene types. Snow is really an object, not a type of scene, but we are
not aware of any work on recognizing snow in the object detection
literature.

We thus begin by assembling a large-scale realistic image dataset,
and test a variety of modern classification techniques on the
problem of snowy scene detection. We use a labeled subset of this
dataset to train classifiers and to test their performance, and then
apply these classifiers to the problem of generating satellite-like
snowfall maps using image analysis on geo-tagged, time-stamped
Flickr photos.

\subsubsection{Single Image classification}

We used a variety of  visual features  for classifying
whether an image contains fallen snow. We used Support Vector
Machines for classification, choosing kernels based on the feature
type. 

We tested these approaches to detecting snow on our dataset of
10,000 hand-labeled images. We split this set into a training set of
8,000 images and a test set of 2,000 images, sampled to have an
 equal proportion of snow and non-snow images (so
that the accuracy of a random baseline is 50\%).
Table~\ref{tab:snow} presents the results. We observe that all of the
features perform significantly better than a random baseline. 
Gist, Color Histograms and Tiny Image all give very similar accuracies, within a half
percentage point of 74\%. Spatial Moments and
LBP  features perform
slightly better at 76.2\% and 77.0\%. We also tested a combination of all 
features by learning a second-level linear SVM on the output of the
five SVMs; this combination performed significantly better than any single feature,
at 80.5\%.


Figure~\ref{fig:PR_ROC_snow} shows classification performance in
terms of an ROC curve, as well as a
precision-recall curve in which the task is to retrieval photos
containing snow. The precision-recall curve shows that at about 20\%
recall, precision is very near to 100\%, while even at 50\% recall,
precision is close to 90\%.  This is a nice feature because in many
applications, it may not be necessarily to correct classify all
images, but instead to find some images that most likely contain a
subject of interest.
%
To give a sense for the difficulty and failure modes of our dataset,
we show a random sample of correct and incorrect classification results
in Figure~\ref{fig:fp}.

\begin{figure*}[th!]
\begin{center}
\vspace{-16pt}
\begin{tabular}{cc}
 \includegraphics[width=0.4\textwidth]{figs/ROC-curves.jpg} &
\includegraphics[width=0.4\textwidth]{figs/PR-curves.jpg} \\
\end{tabular}
\end{center}
\vspace{-8pt}
\caption{
Snow classification results for different features and combinations, in terms of {\textit{(left):}} ROC curves for the task of classifying snow vs. non-snow images; and 
{\textit{(right):}} Precision-Recall curves for the task of retrieving snow images.
}
\label{fig:PR_ROC_snow}
\end{figure*}

\begin{table}
\begin{center}
{\footnotesize{
\begin{tabular}{|l|c|c|}
\hline 
Feature & Kernel  & Accuracy\tabularnewline
\hline 
\hline 
Random Baseline  & --- & 50.0\%\tabularnewline
\hline 
\hline
Gist & RBF & 73.7\%\tabularnewline
\hline 
Color  & $\chi^2$ & 74.1\%\tabularnewline
\hline
Tiny & RBF & 74.3\%\tabularnewline
\hline 
Spatial Color Moments & RBF & 76.2\%\tabularnewline
\hline 
Spatial pyramid LBP & RBF &\textbf{77.0\%}\tabularnewline
\hline 
\hline
All features  & linear & \textbf{80.5\%}\tabularnewline
\hline 
CNN& -& \textbf{88.06\%}\tabularnewline
\hline
\end{tabular}
}}
\caption{Performance of different features  for snow detection, all using SVMs for classification. }
\label{tab:snow}
\end{center}
\end{table}



The best performance we had using our traditional visual features using SVM is 80.5\% accuracy.  We also build CNN visual model for snow using Imagenet pre-trained model. We fine-tune our model using our training data. CNN  achieves 88\% accuracy which  outperforms all other features by 7.44\% . Therefore, we used CNN as our visual model for final predictions.  Similar to visual model we also build SVM using only tags as features and our text classifier achieves 87\% accuracy.




We now turn to presenting experimental results for estimating the
geo-temporal distributions of snow.

\subsubsection{Snow prediction on cities}


We first test how well the Flickr data can predict snowfall at a local
level, and in particular for cities in which high-quality
surface-based snowfall observations exist and for which photo density is high.

We choose 4 U.S. metropolitan areas, New York City, Boston, Chicago and
Philadelphia, and try to predict both daily snow presence as well as
the quantity of snowfall.  For each city, we define a corresponding
geospatial bounding box and select the NOAA ground observation stations in that area. 
For example, 
Figure~\ref{tab:city_statistics} shows the the stations 
and the bounding box for 
New York City. We calculate the ground truth daily snow quantity for a city as the average of
the valid 
snowfall values from its stations.

We call any day with a non-zero snowfall or snowcover to be a snow day,
and any other day to be a non-snow day.

Figure~\ref{tab:city_statistics} also presents some basic statistics for
these 4 cities.  All of our experiments involve 4 years (1461 days) of
data from January 2007 through December 2010; we reserve the first two
years for training and validation, and the second two years for
testing.


\begin{figure}
\begin{center}
\includegraphics[width=0.35\textwidth]{plots/nyc_stations.png} 
\end{center}
\begin{center}
{\small{
\newcommand{\spc}{\hspace{2pt}}
\begin{tabular} {|@{\spc}l@{\spc}|@{\spc}r@{\spc}|@{\spc}r@{\spc}|@{\spc}r@{\spc}|@{\spc}r@{\spc}|} 
\hline 
\textbf{} &{NYC}  &{Chicago} &{Boston}&{Philadelphia} \tabularnewline
\hline 
{Mean active Flickr users / day} &{65.6} &{94.9} &{59.7} &{43.7} \tabularnewline
\hline 
{Approx. city area ($km^2$)} &{3,712} &{11,584} &{11,456} &{9,472}  \tabularnewline
\hline 
{User density (avg users/unit area)} &{112.4} &{52.5} &{33.5} &{29.6} \tabularnewline
\hline 
{Mean daily snow (inches)} &{0.28} &{0.82} &{0.70} &{0.35} \tabularnewline
\hline 
{Snow days (snow>0 inches)} &{185} &{418} &{373} &{280} \tabularnewline
\hline 
{Number of obs. stations} &{14} &{20} &{41} &{26} \tabularnewline
\hline 
\end{tabular}}}
\end{center}
\vspace{-12pt}
 \caption {\textit{Top:} New York City geospatial bounding box used to select Flickr photos, and locations of NOAA observation stations. \textit{Bottom:} Statistics about spatial area, photo density, and ground truth for each of the 4 cities.}
\label{tab:city_statistics} 
\end{figure}




\xhdr{Daily snow classification for 4 cities.}
Figure~\ref{fig:city_roc}(a) presents ROC curves for this
daily snow versus non-snow classification task on New York City. The figure compares the likelihood
ratio confidence score from equation~(\ref{eq:conf}) to the baseline
approaches (voting and percentage), using the tag set
${\cal S}$=\{snow, snowy, snowing, snowstorm\}.
The area under the ROC curve (AUC) statistics are 0.929, 0.905, and 0.903 for confidence, percentage, and voting, respectively, 
and the improvement of the confidence method is statistically significant 
with $p=0.0713$ according to the statistical test of~\cite{auc}.
The confidence method also outperforms other methods for the other three cities (not shown due to
space constraints).  ROC curves for all 4 cities using the likelihood
scores are shown in Figure~\ref{fig:city_roc}(b). Chicago has the best
performance and Philadelphia has the worst; a possible explanation
is that Chicago has the most active Flickr users per
day (94.9) while Philadelphia has the least (43.7).

These methods based on presence or absence of tags are simple and very
fast, but they have a number of disadvantages, including that the tag
set must be manually chosen and that negative correlations between
tags and phenomena are not considered.
We thus tried training a classifier to learn these relationships automatically.
For each day in each city, we produce a single binary feature vector indicating whether 
or not a given tag was used on that day. Also we tried to build classifiers trained based on our likelihood ratio computed based on tags or our visual model predictions. Table~\ref{tab:city_conf_tag_vision} shows the results for these classifiers. Best prefromance obtained when we combine the confidence scores of tags and visual model based on CNN.


\begin{figure*}
%\begin{center}
\hspace{-0.25in}
\small{
\begin{tabular}{@{}c@{}c@{}c@{}c@{}}
\includegraphics[width=0.50\textwidth,clip,trim=0.4in 0 0.8in 0]{plots/nyc_snow_ROC.png} &
\includegraphics[width=0.50\textwidth,clip,trim=0.4in 0 0.8in 0]{plots/city_cmp_snow_ROC.png} 
(a) & (b)  
\end{tabular}
}
%\end{center}
\vspace{-6pt}
\caption{ROC curves for binary snow predictions: (a) ROC curves for New York City, comparing likelihood
ratio confidence score to voting and percentage approaches, (b) ROC curves for 4 cities using the likelihood
scores}
\label{fig:city_roc}
\vspace{-6pt}
\end{figure*}








\begin{table*} 
 \caption {\textbf{Results for Confidence score model using tags and visual classifiers for our 4 cities .}}
\label{tab:city_conf_tag_vision} 
\begin{center}
{
\begin{tabular} {|c|c|c|c|c|c|} 
\hline 
City &  baseline & tags  &  tag confidence  &  vision conf & tags conf and vision conf \tabularnewline
\hline 

{NYC} & 85\% & 85.75 \% &90.4241 \%&90.2873 \% &92.3393 \%\tabularnewline
\hline 
{Chicago)} &72.80\% & 93.5616 \% &94.1176 \% &93.1601 \% &95.0752 \%  \tabularnewline
\hline 
{Boston} & 75.60\%& 90.5479 \% &89.1781 \%&85.2055 \% & 91.2329 \% \tabularnewline
\hline 
{Philly} & 80.50\% & 85.34\% & 89.1929 \% &85.0889 \%	 & 89.1929 \%  \tabularnewline
\hline 

\end{tabular}}
\end{center}
\vspace{-12pt}
\end{table*}

\subsubsection{{Continental-scale snow prediction}}
Predicting snow for individual cities is of limited practical use because accurate meteorological data already exists for these highly populated areas.
Here we ask whether phenomena can be
monitored at a continental scale, a task for which existing data
sources are less complete and accurate.  We use the photo data and
ground truth described in Section~\ref{sec:results}, although for the
experiments presented in this paper we restrict our dataset to North
America (which we defined to be a rectangular region spanning from 10
degrees north, -130 degrees west to 70 degrees north, -50 degrees
west). (We did this because Flickr is a dominant photo-sharing site in
North America, while other regions have other popular
sites --- e.g. Fotolog in Latin America and Renren in China.)  

The spatial resolution of the NASA satellite ground truth datasets is 0.05 degrees
latitude by 0.05 degrees longitude, or about $5 \times 5 km^2$ at the
equator.  (Note that the surface area of these bins is
non-uniform because lines of longitude get closer together near the
poles.)  However, because the number of photos uploaded to Flickr on
any particular day and at any given spatial location is relatively
low, and because of imprecision in Flickr geo-tags, we produce
estimates at a coarser resolution of 1 degree square, or roughly $100
\times 100 km^2$. To make the NASA maps comparable, we downsample them
to this same resolution by averaging the high confidence observations within the coarser bin.
%We then threshold the confidence and snow cover percentages to annotate
%each bin with one of three ground truth labels: 
%%
%\begin{packed_itemize}
%\item[---] Snow bin, if confidence is above 90 and coverage above 80,
%\item[---] Non-snow bin, if confidence is above 90 and coverage is 0,
%\item[---] Unknown bin, otherwise.
%\end{packed_itemize}
%%

%Figure ~\ref{fig:snowcurve} shows the precision and recall curve of snow prediction in continental-scale.
% bar plot of 2 places
\begin{figure}
%{\small{
\begin{center}

\includegraphics[width=0.23\textwidth]{figs/PR-snow.jpg}
\includegraphics[width=0.23\textwidth]{figs/PR-nonsnow.jpg}
%\includegraphics[width=0.5\textwidth,height=1.4in,clip,trim=0 0.5in 0in 0.6in]{plots/chicago_noaa_vs_prediction_prev_3.png} 

\end{center}
%}}
\vspace{-24pt}
\caption{Precision and recall curve of snow prediction (left) and nonsnow (right) in continental scale.}
\label{fig:snowcurve}
\vspace{-12pt}
\end{figure}

% bar plot of 2 places
%\begin{figure}
%%{\small{
%\begin{center}
%
%\includegraphics[width=0.5\textwidth]{nonsnowcurve.jpg}
%
%%\includegraphics[width=0.5\textwidth,height=1.4in,clip,trim=0 0.5in 0in 0.6in]{plots/chicago_noaa_vs_prediction_prev_3.png} 
%
%\end{center}
%%}}
%\vspace{-24pt}
%\caption{Precision and recall curve of nonsnow prediction in continental scale.}
%\label{fig:nonsnowcurve}
%\vspace{-12pt}
%\end{figure}
Figure ~\ref{fig:snowcurve} shows the precision and recall curve of snow prediction in continental-scale.
Here we limit our predictions for the bins which have photos taken at that time and location, we do this by keeping the bins have ground truth and photos at the same time. 
We computed our confidence scores based on tags and image-classification, then we trained simple decision tree to learn the correct thresholds to make final prediction. We achieve almost 0.5\% over the baseline (cutting the error rate by more than 20\%), the baseline in our case is the majority class which predicts now snow all the time.  
  

%increases over the baseline (98.0\% - majority class which predicts now snow all the time).   


%We achieve 98.4\% by combining likelihood scores for tags and visual features compare to the 98.0\% for majority class classifier in this case. We cutting the error rate by more than 20%  


\subsection{Vegetation case}

\subsubsection{Single image classification}


Using the method we describe in Section ~\ref{sec:method}, we train and test the vision model on our hand-labeled data set.
There are 4000 images in training set and 2000 images in testing set. In both training and testing set, the number of positive and negative images are the same. Here we present the results on image classification level.


\begin{table} 
 \caption {\textbf{Results for our  visual models for vegetation.}}
\label{tab:veg_img_classifier} 
\begin{center}
{
\begin{tabular} {|c|c|} 
\hline 
Visual feature &  Accuracy \tabularnewline
\hline 
\hline
Random Baseline & 50.00\%\tabularnewline
\hline
\hline
Color SIFT & 78.10\%\tabularnewline
\hline 
Color GIST & 82.58\% \tabularnewline
\hline 
\hline
SIFT and GIST& 85.9\% \tabularnewline
\hline 
CNN\% &  88.0\%\tabularnewline
\hline 
\end{tabular}
}
\end{center}
\end{table}




\subsubsection{Vegetation coverage over time and space}
We consider north America area has more images uploaded to photo-sharing website, and is also where Ecologists in the US would be interested in the changing color of vegetation. 
%In our work, we define north America as latitude 10$^{\circ}$ to 70$^{\circ}$ and longitude -130$^{\circ}$ to -50$^{\circ}$.


We combine all the evidence over space and time in North America from 2007 to 2010. We compute confidence score described in Section~\ref{sec:method}. 
%Confidence score is measuring the ratio of log likelihood of being a vegetation bin or not at each time period. All the prior statistics are learnt from satellite ground truth in 2007 and 2008.
The prior probability of a place being covered by vegetation at some time is 75.2\%.
%(I doubt this number. This is because the ground truth put too many bins in gray area while there are too many non-green bins on the north that we don't really care.) 
For an image taken from a place covered by green vegetation at that time, the probability of this image being a green image is 27.18\%. On the other hand, it's only 3.03\% probability to see a green image in a place not covered by enough green vegetation at that time.

%Now we present the performance of vegetation detection in north America.

%\hfill \break
%\hfill \break
While the satellite has ground truth for 87594 bins in North America, our method predicts 61602 bins (70.3\% in quantity). Moreover, about 20\% of satellite ground truth locate in north Canada. On the other hand, our data is from users in social media. So our prediction focus on more populated locations or places people like to visit such as natrual scenery.

In North America, the overall accuracy of our method is 93.2\% comparing to the 86.6\% majority baseline. The precision of green bins is 98.8\% and the precision of non-green bins is 68.2\%. Recall of green bins is 93.3\% and recall of non-green bins is 92.5\%.

%snowcurve.pdf
%curvevege.jpg
% precision-recall of vege
Figure ~\ref{fig:curvevege} shows the precision and recall curve of snow prediction in continental-scale.
% bar plot of 2 places
\begin{figure}
%{\small{
\begin{center}

\includegraphics[width=0.5\textwidth]{curvevege.jpg}

%\includegraphics[width=0.5\textwidth,height=1.4in,clip,trim=0 0.5in 0in 0.6in]{plots/chicago_noaa_vs_prediction_prev_3.png} 

\end{center}
%}}
\vspace{-24pt}
\caption{Precision and recall curve of vegetation prediction in continental scale.}
\label{fig:curvevege}
\vspace{-12pt}
\end{figure}

%
%# of bins gt shows nongreen = 67571
%overlap of gt and predi both shows nongreen = 1085
%accuracy = 0.016057184295
%# of bins predi shows green = 42317
%overlap of gt and predi both shows green = 7061
%accuracy = 0.166859654512
%# of bins that both have data = 8741
%overlap = 8146
%accuracy = 0.931929985128
%# of bins that predi have data = 61602
%# of bins that gt have data = 87594
%ratio = 0.703267347079
%in bins both have data, # of predi as gr = 7149
%overlap = 7061
%precision of gr = 0.987690586096
%in bins both have data, # of predi as nongr = 1592
%overlap = 1085
%precision of nongr = 0.681532663317
%in bins both have data, # of gt as gr = 7568
%overlap = 7061
%recall of gr = 0.933007399577
%in bins both have data, # of gt as nongr = 1173
%overlap = 1085
%recall of nongr = 0.924978687127
%
%
%baseline = 0.865804827823


%
%***confusion matrix maps and data
%
%\begin{table}[b]
%\begin{center}
%{\footnotesize{
%\begin{tabular}{|l|c|c|}
%\hline 
%--- & \multicolumn{2}{c|}{Ground truth}\tabularnewline
%\hline
%--- & Greenery & Non-Greenery\tabularnewline
%\hline 
%\hline 
%%Random Baseline  & --- & 50.0\%\tabularnewline
%%\hline 
%%\hline
%Greenery & 2580 & 36\tabularnewline
%\hline 
%Non-Greenery & 353 & 400\tabularnewline
%\hline
%\end{tabular}
%}}
%\caption{Confusion matrix of vegetation detection in 2007}
%\label{tab:confusionmatrix}
%\end{center}
%\end{table}
%(This table looks terrible. Hope there is a way to make it prettier.)
%(??? Result is presented in video. Is a map necessary here? or a better way?)


\begin{figure*}[th]
{\small{
\begin{center}
\begin{tabular}{@{}c@{\,\,\,}c@{\,\,\,}c@{\,\,\,}c@{\,\,\,}c@{\,\,\,}}
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/1.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/2.jpg} &
\includegraphics[height=1in]{imggrid/falseposi/3.jpg} &
\includegraphics[height=1in]{imggrid/falseposi/4.jpg} &
\includegraphics[height=1in]{imggrid/falseposi/5.jpg} \\
%\multicolumn{5}{c}{(a) Random positive images in vegetation dataset} \\
\\[-6pt]
\hline
\\[-6pt]
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/6.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/7.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/8.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/9.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/10.jpg} \\
%\multicolumn{5}{c}{(a) Random positive images in vegetation dataset} \\ 
\\[-6pt]
\hline
\\[-6pt]
\includegraphics[height=1in]{imggrid/falseposi/11.jpg} &
\includegraphics[height=1in]{imggrid/falseposi/12.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/13.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/14.jpg} &
\includegraphics[height=1in]{imggrid/falseposi/15.jpg} \\
%\multicolumn{5}{c}{(c) Random false negatives (snow images classified as non-snow)} \\ 
\\[-6pt]
\hline
\\[-6pt]
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/16.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/17.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/18.jpg} &
\includegraphics[height=1in]{imggrid/falseposi/19.jpg} &
\includegraphics[width=0.19\textwidth]{imggrid/falseposi/20.jpg} \\
%\multicolumn{5}{c}{(b) Random negative images in vegetation dataset} \\
\end{tabular}
\end{center}
}}
\caption{In vegetation detection over North America in 2009 and 2010, among all false positive bins, there are ... images that are predicted as greenery. And these images are the reason these bins are predicted as green. Here are some random selected examples of the green images.}
\label{fig:falseposi}
\end{figure*}

%****explain errors

Generally, all the false negative error is due to the sparseness of data. While not enough images are collected at certain location during some time, there is either no green image found or green images are too few compare to the quantity of non-green images. On the other hand, false positive error is rare (less than 1\%) and complex. We found most images in the false positive bins are actually green vegetation images. ( here we need some more explanation) In figure ~\ref{fig:falseposi}, we show some examples of images in false positive bins.



\subsubsection{Performance at single place over time}
Figure ~\ref{fig:placeinbar} shows vegetation coverage of 6 places over 2009 and 2010. Prediction results on top usually have more data available than groud truth on the bottom. We use public sharing Flickr images. These images are more likely taken from more populated or more popular locations. The satellite ground truth 


% bar plot of 2 places
\begin{figure}
%{\small{
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=0.14\textwidth]{bar/8560.jpg} &
\includegraphics[width=0.14\textwidth]{bar/8561.jpg} &
%\includegraphics[width=0.5\textwidth,height=1.4in,clip,trim=0 0.5in 0in 0.6in]{plots/chicago_noaa_vs_prediction_prev_3.png} 
\includegraphics[width=0.14\textwidth]{bar/8881.jpg} \\
\includegraphics[width=0.14\textwidth]{bar/8911.jpg} &
\includegraphics[width=0.14\textwidth]{bar/9705.jpg} &
%\includegraphics[width=0.5\textwidth,height=1.4in,clip,trim=0 0.5in 0in 0.6in]{plots/chicago_noaa_vs_prediction_prev_3.png} 
\includegraphics[width=0.14\textwidth]{bar/10816.jpg} \\
%(a) & (b)\\
\\
\end{tabular}
\end{center}
%}}
\vspace{-24pt}
\caption{Yellow bars show non-greenery at that time. Blue bars represent greenery. Prediction results on top shows 6 random places comparing to satellite ground truth. The ground truth on the bottom tends to disappear when leaves are turning yellow or green.}
\label{fig:placeinbar}
\vspace{-12pt}
\end{figure}

\subsubsection{Single time over places}

Sample maps are presented in figure ~\ref{fig:map}. 
%(I should print an outline of continent as background of these maps.)

% bar plot of 2 places
\begin{figure}
%{\small{
\begin{center}
\begin{tabular}{cccc}
\includegraphics[width=0.10\textwidth]{map/72.png} &
\includegraphics[width=0.10\textwidth]{map/77.png} &
\includegraphics[width=0.10\textwidth]{map/78.png} &
%\includegraphics[width=0.5\textwidth,height=1.4in,clip,trim=0 0.5in 0in 0.6in]{plots/chicago_noaa_vs_prediction_prev_3.png} 
\includegraphics[width=0.10\textwidth]{map/79.png}  \\
Mar, 6th&May, 25th&Jun, 10th&Jun, 26th\\
\includegraphics[width=0.10\textwidth]{map/82.png} &
\includegraphics[width=0.10\textwidth]{map/83.png} &
\includegraphics[width=0.10\textwidth]{map/84.png} &
%\includegraphics[width=0.5\textwidth,height=1.4in,clip,trim=0 0.5in 0in 0.6in]{plots/chicago_noaa_vs_prediction_prev_3.png} 
\includegraphics[width=0.10\textwidth]{map/91.png}  \\
Aug, 13th&Aug, 29th&Sep, 14th&Dec 19th\\
\\
\end{tabular}
\end{center}
%}}
\vspace{-24pt}
\caption{%On two 16-days time period, end of August on the left and during Christmas on the right, we show the confusion matrix map here. Orange bins are true positive; yellow bins are false negative; blue bins are false positive; and black bins are true negative.
We use prediction results to recreate vegetation coverage maps for each 16-days period. There are 8 maps picked in 2010. The dates under each map are the starting date of each 16-days period.}
\label{fig:map}
\vspace{-12pt}
\end{figure}


\section{discussion and conclusion}




Big wrap-up, lots of ideas for future work.

In this paper, we show a real case of using public-sharing data to estimate ecology information.
We used get this information only from some institution with the help of satellite.


In this paper, we propose using photo-sharing social media sites as a
means of observing the state of the natural world, by automatically
recognizing specific types of scenes and objects in large-scale social
image collections. This work is an initial step towards a long-term
goal of monitoring important ecological events and trends through
online social media.  Our study shows that snowy scene recognition is
not nearly as easy a problem as one might expect, when applied to realistic
consumer images; our best result using
modern vision techniques gives 81\% accuracy. Nevertheless, as a proof-of-concept
we demonstrated that this recognition accuracy still yields a reasonable
map that approximates observations from satellites.
%However, using the current modern vision techniques to solve
%this problem, we are able to recreate the satellite map.  
We also test recognition algorithms on their ability to recognize a
particular species of flower, the California Poppy. In future work, we
plan to combine evidence from tags and other metadata with visual
features for more accurate estimates, and to develop novel techniques
for these challenging recognition problems. More generally, we hope
the idea of observing nature through photo-sharing websites will help
spark renewed interest in recognizing natural and ecological phenomenon in
consumer images.
%=======
%In this paper, we   propose using massive amount of latent visual information uploaded to social media as source  observing the state of the natural world by recognizing specific types of scenes and objects in large-scale social image collections. This work can be considered as  preliminary step towards long-term goal to understand the ecology phenomena using online social media.  Our study shows that snowy scene recognition  is not an easy problem as been expected. Best  results obtained using modern vision techniques is around 81\% accuracy which is not  high accuracy. However, using  the current modern vision techniques to solve this problem, we are able to recreate the satellite map.  We also  show the   current  recognition algorithms able to  detect a particular species of flower, the California
%Poppy at accuracy 72\%. 
%In future work we plan to find more sophisticated computer vision techniques for  these problems. Also, we plan to combine this work  with  the work of Zhang \textit{et al}~\cite{ecology2012www} which is based on textual information to improve the results. Also, we plan to study more ecological phenomena like migration patterns of wildlife.
%>>>>>>> .r2693

%% In future work we plan to find more sophisticated computer vision
%% techniques for these problems. Also, we plan t combine this work based
%% on visual information with the work of Zhang \textit{et
%%   al}~\cite{ecology2012www} which is based on textual information to
%% improve the results. Also, we plan to study more ecological phenomena
%% like migration patterns of wildlife.



In this paper, we propose using the massive collections of
user-generated photos uploaded to social sharing websites as a source
of observational evidence about the world, and in particular as a way
of estimating the presence of ecological phenomena. As a first step
towards this long-term goal, we used a collection of 150 million
geo-tagged, timestamped photos from Flickr to estimate snow 
cover and greenery, and compared these estimates to fine-grained
ground truth collected by earth-observing satellites and ground stations. We compared
several techniques for performing the estimation from noisy, biased
data, including simple voting mechanisms and a Bayesian likelihood
ratio. We also tested several possible improvements to these basic
%%hp cr: removed "multi-language tag sets" as we did not report it in
%%this version
methods, including using temporal smoothing
%%methods, including using temporal smoothing, multi-language tag sets,
and machine learning to improve the accuracy of estimates. We found
that while the recall is relatively low due to the sparsity of photos
on any given day, the precision can be quite high, suggesting that
mining from photo sharing websites could be a reliable source of
observational data for ecological and other scientific research. In
future work, we plan to study additional features including using
more sophisticated computer vision techniques to analyze visual content. Also we plan to study a variety of other ecological phenomena,
including those for which high quality ground truth is not available,
such as migration patterns of wildlife and the distributions of
blooming flowers.






% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

%
%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.
%
%% you can choose not to have a title for an appendix
%% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.
%
%
%% use section* for acknowledgment
%\ifCLASSOPTIONcompsoc
%  % The Computer Society usually uses the plural form
%  \section*{Acknowledgments}
%\else
%  % regular IEEE prefers the singular form
%  \section*{Acknowledgment}
%\fi
%
%
%The authors would like to thank...
%
%
%% Can use something like this to put references on a page
%% by themselves when using endfloat and the captionsoff option.
%\ifCLASSOPTIONcaptionsoff
%  \newpage
%\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to {\LaTeX}}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}{Michael Shell}
Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{John Doe}
Biography text here.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiographynophoto}{Jane Doe}
Biography text here.
\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


